{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83de6985",
   "metadata": {},
   "source": [
    "An outlier may occur due to the variability in the data, or due to experimental error/human error.\n",
    "\n",
    "They may indicate an experimental error or heavy skewness in the data(heavy-tailed distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4548b8",
   "metadata": {},
   "source": [
    "In statistics, we have three measures of central tendency namely Mean, Median, and Mode. They help us describe the data.\n",
    "\n",
    "Mean is the accurate measure to describe the data when we do not have any outliers present.\n",
    "\n",
    "Median is used if there is an outlier in the dataset.\n",
    "\n",
    "Mode is used if there is an outlier AND about ½ or more of the data is the same.\n",
    "\n",
    "‘Mean’ is the only measure of central tendency that is affected by the outliers which in turn impacts Standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d337fdc",
   "metadata": {},
   "source": [
    "# Detecting Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32f7e01",
   "metadata": {},
   "source": [
    "Below are some of the techniques of detecting outliers\n",
    "\n",
    "Boxplots\n",
    "\n",
    "Z-score\n",
    "\n",
    "Inter Quantile Range(IQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c5e6b",
   "metadata": {},
   "source": [
    "any data point whose Z-score falls out of 3rd standard deviation is an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9651016b",
   "metadata": {},
   "source": [
    "# By  Z score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07ed479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With sid 3 lets see the stats \n",
    "#From scipy import stats \n",
    "#zscore=(x_mean)/std=>you have seen  this is Standard Scaler \n",
    "\n",
    "#Z=(x-mean)/std\n",
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "z_score=zscore(df[['age','height','ap_hi','weight','ap_lo']])\n",
    "abs_z_score=np.abs(z_score)#apply the formula and you get the scaled data \n",
    "filtering_entry=(abs_z_score < 3).all(axis=1)\n",
    "df=df[filtering_entry]\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3353f3",
   "metadata": {},
   "source": [
    "# By IQR -> Intel Quantile Range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6369873d",
   "metadata": {},
   "source": [
    "data points that lie 1.5 times of IQR above Q3 and below Q1 are outliers. This shows in detail about outlier treatment in Python.\n",
    "\n",
    "steps:\n",
    "\n",
    "\n",
    "Sort the dataset in ascending order\n",
    "\n",
    "\n",
    "calculate the 1st and 3rd quartiles(Q1, Q3)\n",
    "\n",
    "\n",
    "compute IQR=Q3-Q1\n",
    "\n",
    "\n",
    "compute lower bound = (Q1–1.5*IQR), upper bound = (Q3+1.5*IQR)\n",
    "\n",
    "\n",
    "loop through the values of the dataset and check for those who fall below the lower bound and above the upper bound and mark them as outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb4b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Outliers in  a Empty List\n",
    "outliers = []\n",
    "def detect_outliers_iqr(data):\n",
    "    data = sorted(data)\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    # print(q1, q3)\n",
    "    IQR = q3-q1\n",
    "    lwr_bound = q1-(1.5*IQR)\n",
    "    upr_bound = q3+(1.5*IQR)\n",
    "    # print(lwr_bound, upr_bound)\n",
    "    for i in data: \n",
    "        if (i<lwr_bound or i>upr_bound):\n",
    "            outliers.append(i)\n",
    "    return outliers# Driver code\n",
    "sample_outliers = detect_outliers_iqr(sample)\n",
    "print(\"Outliers from IQR method: \", sample_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79880196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Upper Value\n",
    "#1st quantile \n",
    "q1=data.quantile(0.25)\n",
    "#3rd quantile\n",
    "q3=data.quantile(0.75)\n",
    "#IQR \n",
    "IQR =q3  - q1\n",
    "\n",
    "\n",
    "st_high=(q3.SkinThickness + (1.5 * IQR.SkinThickness))\n",
    "print(st_high)\n",
    "\n",
    "\n",
    "index=np.where(data['SkinThickness']>st_high)\n",
    "\n",
    "\n",
    "data=data.drop(data.index[index])\n",
    "\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "\n",
    "data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2474c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Lower Value\n",
    "bp_low=(q1.BloodPressure - (1.5 * IQR.BloodPressure))\n",
    "print(bp_low)\n",
    "\n",
    "index=np.where(data['BloodPressure']<bp_low)\n",
    "\n",
    "\n",
    "\n",
    "data=data.drop(data.index[index])\n",
    "\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "\n",
    "data.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b17cf38",
   "metadata": {},
   "source": [
    "# Below are some of the methods of treating the outliers\n",
    "\n",
    "    Trimming/removing the outlier\n",
    "    Quantile based flooring and capping\n",
    "    Mean/Median imputation\n",
    "    5.1 Trimming/Remove the outliers\n",
    "In this technique, we remove the outliers from the dataset. Although it is not a good practice to follow.\n",
    "\n",
    "Python code to delete the outlier and copy the rest of the elements to another array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261c5690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trimming\n",
    "for i in sample_outliers:\n",
    "    a = np.delete(sample, np.where(sample==i))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eae920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(sample), len(a))\n",
    "#The outlier ‘101’ is deleted and the rest of the data points are copied to another array ‘a’.\n",
    "\n",
    "#5.2 Quantile based flooring and capping\n",
    "# In this technique, the outlier is capped at a certain value above the 90th percentile value or floored at a factor below the 10th percentile value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6452ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample= [15, 101, 18, 7, 13, 16, 11, 21, 5, 15, 10, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eddf52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##for i in sample_outliers:\n",
    "   # a = np.delete(sample, np.where(sample==i))\n",
    "#print(a)\n",
    "# print(len(sample), len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "896035cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New array: [15.  20.7 18.   7.2 13.  16.  11.  20.7  7.2 15.  10.   9. ]\n"
     ]
    }
   ],
   "source": [
    "# Computing 10th, 90th percentiles and replacing the outliers\n",
    "import numpy as np\n",
    "tenth_percentile = np.percentile(sample, 10)\n",
    "ninetieth_percentile = np.percentile(sample, 90)\n",
    "# print(tenth_percentile, ninetieth_percentile)\n",
    "b = np.where(sample<tenth_percentile, tenth_percentile, sample)\n",
    "b = np.where(b>ninetieth_percentile, ninetieth_percentile, b)\n",
    "# print(\"Sample:\", sample)\n",
    "print(\"New array:\",b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae3d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As the mean value is highly influenced by the outliers, it is advised to replace the outliers with the median value.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "median = np.median(sample)# Replace with median\n",
    "for i in sample_outliers:\n",
    "    c = np.where(sample==i, 14, sample)\n",
    "print(\"Sample: \", sample)\n",
    "print(\"New array: \",c)\n",
    "# print(x.dtype)\n",
    "#Visualizing the data after treating the outlier\n",
    "\n",
    "plt.boxplot(c, vert=False)\n",
    "plt.title(\"Boxplot of the sample after treating the outliers\")\n",
    "plt.xlabel(\"Sample\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e274220f",
   "metadata": {},
   "source": [
    "# There are so many types of feature transformation methods, we will talk about the most useful and popular ones.\n",
    "\n",
    "\n",
    "Standardization\n",
    "\n",
    "\n",
    "Min — Max Scaling/ Normalization\n",
    "\n",
    "\n",
    "Robust Scaler\n",
    "\n",
    "\n",
    "Logarithmic Transformation\n",
    "\n",
    "\n",
    "Reciprocal Transformation\n",
    "\n",
    "\n",
    "Square Root Translation\n",
    "\n",
    "\n",
    "Box-Cox Transformation\n",
    "\n",
    "\n",
    "Exponential Transformation\n",
    "\n",
    "\n",
    "Johnson transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f96dbd",
   "metadata": {},
   "source": [
    "# Standardization\n",
    "\n",
    "Standardization should be used when the features of the input dataset have large differences between ranges or when they are measured in different measurements units like Height, Weight, Meters, Miles, etc.\n",
    "\n",
    "We bring all the variables or features to a similar scale. Where the mean is 0 and the Standard Deviation is 1.\n",
    "\n",
    "In Standardization, we subtract feature values by their mean and then divide by standard deviation which gives exactly standard normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb13c4fd",
   "metadata": {},
   "source": [
    "# Min Max Scaler\n",
    "\n",
    "In simple terms, min-max scaling brings down feature values to a range of 0 to 1. Until we specify the range we want it to be scaled down to.\n",
    "\n",
    "In Normalization, we subtract the feature value by its minimum value and then divide it by the range of features (range of feature= maximum value of feature — minimum value of feature)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a625d08",
   "metadata": {},
   "source": [
    "# Robust to Outliers\n",
    "\n",
    "\n",
    "If the dataset has too many outliers, both Standardization and Normalization can be hard to depend on, in such case you can use Robust Scaler for feature scaling.\n",
    "\n",
    "You can also say Robust Scaler is robust to outliers ?.\n",
    "\n",
    "It scales values using median and interquartile range therefore it doesn’t get affected by very large or very small values of features.\n",
    "\n",
    "The robust scaler subtracts feature values by their median and then divides by its IQR.\n",
    "\n",
    "25th percentile = 1st quartile\n",
    "\n",
    "\n",
    "50th percentile = 2nd quartile (also called the median)\n",
    "\n",
    "\n",
    "75th percentile = 3rd quartile\n",
    "\n",
    "\n",
    "100th percentile = 4th quartile (also called the maximum)\n",
    "\n",
    "\n",
    "IQR= Inter Quartile Range\n",
    "\n",
    "\n",
    "IQR= 3rd quartile — 1st quartile\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373b7ea9",
   "metadata": {},
   "source": [
    "# 1. Log Transformation — right skewed data\n",
    "\n",
    "When the data sample follows the power law distribution, we can use log scaling to transform the right skewed distribution into normal distribution. To achieve this, simply use the np.log() function. In this dataset, most variables fall under this category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554a13a4",
   "metadata": {},
   "source": [
    "In the Logarithmic Transformation, we will apply log to all values of features using NumPy and store it in the new feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79da527e",
   "metadata": {},
   "source": [
    "Using Log Transformation doesn’t seem to fit very well in this dataset, it even worsens the distribution by making data left-skewed. So we have to rely on other methods to achieve normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12357003",
   "metadata": {},
   "source": [
    "# B. Reciprocal Transformation\n",
    "In Reciprocal Transformation, we divide each value of a feature by 1(reciprocal) and store it in the new feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cb12d2",
   "metadata": {},
   "source": [
    "Reciprocal Transformation doesn’t work well with this data, It doesn’t give normal distribution instead it made data even more right-skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677449d2",
   "metadata": {},
   "source": [
    "# C. Square Root Translation\n",
    "In square root transformation, we raise the values of feature to the power of fraction(1/2) to achieve the square root of a value. We can also use NumPy for this transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d65a9f",
   "metadata": {},
   "source": [
    "Square root transformation seems to perform better than reciprocal and log transformation with this data but yet it is a bit left-skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26b04f1",
   "metadata": {},
   "source": [
    "# D. Box-Cox Transformation\n",
    "Box-Cox transformation is one of the most useful scaling techniques to transfer data distribution in a normal distribution.\n",
    "\n",
    "The Box-Cox transformation can be defined as:\n",
    "\n",
    "T(Y)=(Y exp(λ)−1)/λ\n",
    "\n",
    "Where Y is the response variable and λ is the transformation parameter. λ varies from -5 to 5. In the transformation, all values of λ are considered and the optimal value for a given variable is selected.\n",
    "\n",
    "We can calculate box cox transformation using stats from the SciPy module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9285c13e",
   "metadata": {},
   "source": [
    "So far box cox transformation seems to be the best fit for the age feature to transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a474c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['df_exponential'] = df.Age**(1/1.2)\n",
    "plot_data(df, 'df_exponential')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4fe4b4",
   "metadata": {},
   "source": [
    "# Guassian Transformation\n",
    "\n",
    "What is Guassian Transformation?\n",
    "If our features are not normally distributed then we use mathematical operations to convert that into normal or guassian distribution This is called guasian Transformation.\n",
    "\n",
    "Some machine learning algorithms like linear and logistic assume that the features are normally distributed. They give us Accuracy and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00582e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intern at Pranathi \n",
    "#Student of DataTrained- Saurav\n",
    "#Date - 30- March - 2023\n",
    "#Time  - 10:10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
