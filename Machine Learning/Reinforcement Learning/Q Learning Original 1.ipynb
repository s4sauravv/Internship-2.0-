{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "831bf9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81a2ff35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>05/01/2023</td>\n",
       "      <td>19.16</td>\n",
       "      <td>19.25</td>\n",
       "      <td>18.46</td>\n",
       "      <td>18.55</td>\n",
       "      <td>2,356,640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04/28/2023</td>\n",
       "      <td>18.92</td>\n",
       "      <td>19.72</td>\n",
       "      <td>18.88</td>\n",
       "      <td>19.29</td>\n",
       "      <td>2,836,576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date   Open   High    Low  Close     Volume\n",
       "0  05/01/2023  19.16  19.25  18.46  18.55  2,356,640\n",
       "1  04/28/2023  18.92  19.72  18.88  19.29  2,836,576"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the GME stock dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\saura\\Downloads\\Download Data - STOCK_US_XNYS_GME.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b273073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "# Perform any necessary data cleaning, feature engineering, or normalization\n",
    "\n",
    "# Define the state representation and action space\n",
    "def get_state(observation):\n",
    "    # Define the state representation for a given observation\n",
    "    state = [observation['Open'], observation['Close'], observation['Volume']]\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d7abaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = ['Buy', 'Sell', 'Hold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14d09432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Q-table\n",
    "state_space = [i for i in range(len(df))]\n",
    "action_space = [i for i in range(len(action_space))]\n",
    "\n",
    "q_table = np.zeros((len(state_space), len(action_space)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "436903e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-learning algorithm\n",
    "def q_learning(state, action, reward, next_state, alpha, gamma):\n",
    "    # Update the Q-value for a given state-action pair\n",
    "    old_q = q_table[state][action]\n",
    "    next_max_q = np.max(q_table[next_state])\n",
    "    new_q = (1 - alpha) * old_q + alpha * (reward + gamma * next_max_q)\n",
    "    q_table[state][action] = new_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bf5f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Q-learning agent\n",
    "epsilon = 0.9\n",
    "gamma = 0.95\n",
    "alpha = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9f6e272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Total reward = 1.120000000000001\n",
      "Episode 1: Total reward = 3.179999999999996\n",
      "Episode 2: Total reward = 3.0899999999999963\n",
      "Episode 3: Total reward = 2.3900000000000077\n",
      "Episode 4: Total reward = 1.5700000000000003\n",
      "Episode 5: Total reward = 5.129999999999988\n",
      "Episode 6: Total reward = 5.429999999999996\n",
      "Episode 7: Total reward = 2.1099999999999994\n",
      "Episode 8: Total reward = 3.7200000000000024\n",
      "Episode 9: Total reward = 3.1899999999999906\n",
      "Episode 10: Total reward = 7.720000000000002\n",
      "Episode 11: Total reward = 3.3999999999999986\n",
      "Episode 12: Total reward = 5.569999999999997\n",
      "Episode 13: Total reward = 6.769999999999996\n",
      "Episode 14: Total reward = 7.0399999999999885\n",
      "Episode 15: Total reward = 5.670000000000002\n",
      "Episode 16: Total reward = 6.189999999999991\n",
      "Episode 17: Total reward = 7.640000000000001\n",
      "Episode 18: Total reward = 7.969999999999999\n",
      "Episode 19: Total reward = 8.319999999999997\n",
      "Episode 20: Total reward = 7.199999999999999\n",
      "Episode 21: Total reward = 8.319999999999997\n",
      "Episode 22: Total reward = 6.5000000000000036\n",
      "Episode 23: Total reward = 8.319999999999997\n",
      "Episode 24: Total reward = 8.319999999999997\n",
      "Episode 25: Total reward = 8.319999999999997\n",
      "Episode 26: Total reward = 8.319999999999997\n",
      "Episode 27: Total reward = 8.319999999999997\n",
      "Episode 28: Total reward = 8.319999999999997\n",
      "Episode 29: Total reward = 8.319999999999997\n",
      "Episode 30: Total reward = 8.319999999999997\n",
      "Episode 31: Total reward = 7.289999999999996\n",
      "Episode 32: Total reward = 8.319999999999997\n",
      "Episode 33: Total reward = 8.319999999999997\n",
      "Episode 34: Total reward = 8.319999999999997\n",
      "Episode 35: Total reward = 8.319999999999997\n",
      "Episode 36: Total reward = 8.319999999999997\n",
      "Episode 37: Total reward = 8.319999999999997\n",
      "Episode 38: Total reward = 8.319999999999997\n",
      "Episode 39: Total reward = 8.319999999999997\n",
      "Episode 40: Total reward = 8.319999999999997\n",
      "Episode 41: Total reward = 8.319999999999997\n",
      "Episode 42: Total reward = 8.319999999999997\n",
      "Episode 43: Total reward = 8.319999999999997\n",
      "Episode 44: Total reward = 8.319999999999997\n",
      "Episode 45: Total reward = 8.319999999999997\n",
      "Episode 46: Total reward = 8.319999999999997\n",
      "Episode 47: Total reward = 8.319999999999997\n",
      "Episode 48: Total reward = 8.319999999999997\n",
      "Episode 49: Total reward = 8.319999999999997\n",
      "Episode 50: Total reward = 6.979999999999993\n",
      "Episode 51: Total reward = 8.319999999999997\n",
      "Episode 52: Total reward = 8.319999999999997\n",
      "Episode 53: Total reward = 8.319999999999997\n",
      "Episode 54: Total reward = 8.319999999999997\n",
      "Episode 55: Total reward = 8.319999999999997\n",
      "Episode 56: Total reward = 8.319999999999997\n",
      "Episode 57: Total reward = 8.319999999999997\n",
      "Episode 58: Total reward = 7.9999999999999964\n",
      "Episode 59: Total reward = 8.319999999999997\n",
      "Episode 60: Total reward = 8.319999999999997\n",
      "Episode 61: Total reward = 8.319999999999997\n",
      "Episode 62: Total reward = 8.319999999999997\n",
      "Episode 63: Total reward = 8.319999999999997\n",
      "Episode 64: Total reward = 8.319999999999997\n",
      "Episode 65: Total reward = 8.319999999999997\n",
      "Episode 66: Total reward = 8.319999999999997\n",
      "Episode 67: Total reward = 8.319999999999997\n",
      "Episode 68: Total reward = 8.319999999999997\n",
      "Episode 69: Total reward = 8.319999999999997\n",
      "Episode 70: Total reward = 8.319999999999997\n",
      "Episode 71: Total reward = 8.319999999999997\n",
      "Episode 72: Total reward = 8.319999999999997\n",
      "Episode 73: Total reward = 8.319999999999997\n",
      "Episode 74: Total reward = 8.319999999999997\n",
      "Episode 75: Total reward = 8.319999999999997\n",
      "Episode 76: Total reward = 8.319999999999997\n",
      "Episode 77: Total reward = 8.319999999999997\n",
      "Episode 78: Total reward = 8.319999999999997\n",
      "Episode 79: Total reward = 8.319999999999997\n",
      "Episode 80: Total reward = 8.319999999999997\n",
      "Episode 81: Total reward = 8.319999999999997\n",
      "Episode 82: Total reward = 8.319999999999997\n",
      "Episode 83: Total reward = 8.319999999999997\n",
      "Episode 84: Total reward = 8.319999999999997\n",
      "Episode 85: Total reward = 8.319999999999997\n",
      "Episode 86: Total reward = 8.319999999999997\n",
      "Episode 87: Total reward = 8.319999999999997\n",
      "Episode 88: Total reward = 8.319999999999997\n",
      "Episode 89: Total reward = 8.319999999999997\n",
      "Episode 90: Total reward = 8.319999999999997\n",
      "Episode 91: Total reward = 8.319999999999997\n",
      "Episode 92: Total reward = 8.319999999999997\n",
      "Episode 93: Total reward = 8.319999999999997\n",
      "Episode 94: Total reward = 8.319999999999997\n",
      "Episode 95: Total reward = 8.319999999999997\n",
      "Episode 96: Total reward = 8.319999999999997\n",
      "Episode 97: Total reward = 8.319999999999997\n",
      "Episode 98: Total reward = 8.319999999999997\n",
      "Episode 99: Total reward = 8.319999999999997\n"
     ]
    }
   ],
   "source": [
    "for episode in range(100):\n",
    "    state = 0\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    actions = []  # Define the actions list\n",
    "    \n",
    "    while not done:\n",
    "        # Choose the action with the highest Q-value or random action with epsilon probability\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = np.random.choice(action_space)\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "        \n",
    "        # Execute the action and observe the next state and reward\n",
    "        current_price = df.loc[state, 'Close']\n",
    "        next_price = df.loc[state+1, 'Close']\n",
    "        \n",
    "        \n",
    "        if action == 0:  # Buy\n",
    "            reward = next_price - current_price\n",
    "        elif action == 1:  # Sell\n",
    "            reward = current_price - next_price\n",
    "        else:  # Hold\n",
    "            reward = 0\n",
    "        \n",
    "        next_state = state + 1\n",
    "        # Update the Q-table\n",
    "        q_learning(state, action, reward, next_state, alpha, gamma)\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        actions.append(action)  # Append the action to the actions list\n",
    "        \n",
    "        if state == len(df) - 1:\n",
    "            done = True\n",
    "    \n",
    "    print(f\"Episode {episode}: Total reward = {total_reward}\")\n",
    "    epsilon = 0.9 * epsilon\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "664137e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of optimal actions based on a benchmark strategy\n",
    "optimal_actions = []\n",
    "buy_price = None\n",
    "\n",
    "for i in range(len(df)-1):\n",
    "    current_price = df.loc[i, 'Close']\n",
    "    next_price = df.loc[i+1, 'Close']\n",
    "\n",
    "    if current_price < next_price:  # Buy if the price is expected to increase\n",
    "        if buy_price is None:\n",
    "            optimal_actions.append(0)  # Buy\n",
    "            buy_price = current_price\n",
    "        else:\n",
    "            optimal_actions.append(2)  # Hold\n",
    "    elif current_price > next_price:  # Sell if the price is expected to decrease\n",
    "        if buy_price is not None:\n",
    "            optimal_actions.append(1)  # Sell\n",
    "            buy_price = None\n",
    "        else:\n",
    "            optimal_actions.append(2)  # Hold\n",
    "    else:  # Hold if the price is expected to remain the same\n",
    "        optimal_actions.append(2)  # Hold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "723b5ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 0, 2, 2, 1, 0, 2, 2, 2, 2, 1, 0, 1, 0, 1, 0, 2]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2df46fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Profit = 1.8499999999999943\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the agent\n",
    "profits = []\n",
    "buy_price = None\n",
    "for i in range(len(df)-1):\n",
    "    current_price = df.loc[i, 'Close']\n",
    "    next_price = df.loc[i+1, 'Close']\n",
    "    \n",
    "    if actions[i] == 0:  # Buy\n",
    "        buy_price = current_price\n",
    "    elif actions[i] == 1:  # Sell\n",
    "        if buy_price is not None:\n",
    "            profit = current_price - buy_price\n",
    "            profits.append(profit)\n",
    "            buy_price = None\n",
    "    else:  # Hold\n",
    "        pass\n",
    "\n",
    "total_profit = sum(profits)\n",
    "print(f\"Total Profit = {total_profit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc912aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annualized Return = 24.536842105263084\n"
     ]
    }
   ],
   "source": [
    "# Calculate annualized return\n",
    "trading_days_per_year = 252\n",
    "total_trading_days = len(df) - 1\n",
    "annualized_return = (total_profit / total_trading_days) * trading_days_per_year\n",
    "print(f\"Annualized Return = {annualized_return}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4992260d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 57.89473684210527%\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "correct_predictions = sum(actions[i] == optimal_actions[i] for i in range(len(actions)))\n",
    "accuracy = correct_predictions / len(actions) * 100\n",
    "print(f\"Accuracy: {accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3b1ee0",
   "metadata": {},
   "source": [
    "Please note that this code is a basic framework and may need to be modified to match the specifics of your stock trading scenario and the GME dataset. Also, remember that predicting and trading stocks is a complex task and using Q-learning alone may not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f7a2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intern at Pranathi\n",
    "#Student of DataTrained \n",
    "#Date -> 03-05-2023\n",
    "#Time -> 3:17 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
